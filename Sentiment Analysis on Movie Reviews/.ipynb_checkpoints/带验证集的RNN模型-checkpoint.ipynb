{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d530b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentiment Analysis on Movie Reviews'''\n",
    "import math\n",
    "import torch\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698c6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAData(Dataset):\n",
    "    def __init__(self, train):\n",
    "        # 构建数据样本\n",
    "        self.train = train\n",
    "        self.data = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "\n",
    "        if self.train:\n",
    "            # 随机选取80%作为训练集，不可按索引顺序取，数据会不全面\n",
    "            self.data = self.data.sample(frac=0.8, replace=False, random_state=1, axis=0)\n",
    "            # self.data = self.data[:int(self.data.shape[0] * 0.8)]\n",
    "            self.data = self.data.reset_index(drop=True)  # 重新生成索引\n",
    "            ### 正式训练要训练所有数据 ###\n",
    "            # self.data = self.data\n",
    "            self.len = self.data.shape[0]\n",
    "        else:\n",
    "            # 20%作为验证集\n",
    "            self.data = self.data.sample(frac=0.2, replace=False, random_state=1, axis=0)\n",
    "            # self.data = self.data[int(self.data.shape[0] * 0.8):]\n",
    "            self.data = self.data.reset_index(drop=True)  # 重新生成索引\n",
    "            self.len = self.data.shape[0]\n",
    "        self.x_data, self.y_data = self.data['Phrase'], self.data['Sentiment']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据数据索引获取样本\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据长度\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9852cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集验证集数据对象\n",
    "train_set = SAData(train=True)\n",
    "validation_set = SAData(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa32c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "N_CHARS = 128  # ASCII码个数\n",
    "HIDDEN_SIZE = 128\n",
    "N_LAYER = 2\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 10\n",
    "USE_GPU = True\n",
    "N_CLASS = len(set(train_set.y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365ec7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集验证集数据加载对象\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    # num_workers=2\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    dataset=validation_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # 测试集不打乱有利于观察结果\n",
    "    # num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c56da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def phrase2list(phrase):\n",
    "    arr = [ord(c) for c in phrase]  # ord() 返回对应的ASCII码\n",
    "    return arr, len(arr)\n",
    "\n",
    "\n",
    "def create_tensor(tensor):\n",
    "    if USE_GPU:\n",
    "        device = torch.device('cuda:0')\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "def make_tensors(phrase,sentiment):\n",
    "    sequences_and_lengths = [phrase2list(phrase) for phrase in phrase]\n",
    "    phrase_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])\n",
    "    sentiment = sentiment.long()\n",
    "    seq_tensor = torch.zeros(len(phrase_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(phrase_sequences, seq_lengths), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    seq_lengths, prem_idx = seq_lengths.sort(dim=0, descending=True)\n",
    "    seq_tensor = seq_tensor[prem_idx]\n",
    "    sentiment = sentiment[prem_idx]\n",
    "    return seq_tensor.to(device), seq_lengths.to(device), sentiment.to(device)\n",
    "\n",
    "def make_tensor(phrase, sentiment):\n",
    "    sequences_and_lengths = [phrase2list(phrase) for phrase in phrase]  # 名字字符串->字符数组->对应ASCII码\n",
    "    phrase_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths]).cpu()\n",
    "    sentiment = sentiment.long()\n",
    "\n",
    "    # make tensor of name, batchSize x seqLen\n",
    "    seq_tensor = torch.zeros(len(phrase_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(phrase_sequences, seq_lengths)):  # 填充零\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)  # name_sequences不够最大长度的位置补零\n",
    "\n",
    "    # 排序 sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)  # perm_idx表示排完序元素原本的索引\n",
    "    seq_tensor = seq_tensor[perm_idx]  # 对补零后的name_sequences按照长度排序\n",
    "    sentiment = sentiment[perm_idx]\n",
    "\n",
    "    return create_tensor(seq_tensor), create_tensor(seq_lengths), create_tensor(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f9c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensors1(phrase):\n",
    "    sequences_and_lengths = [phrase2list(phrase) for phrase in phrase]\n",
    "    phrase_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])\n",
    "\n",
    "    seq_tensor = torch.zeros(len(phrase_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(phrase_sequences, seq_lengths), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    seq_lengths, prem_idx = seq_lengths.sort(dim=0, descending=True)\n",
    "    seq_tensor = seq_tensor[prem_idx]\n",
    "    _, index = prem_idx.sort(descending=False)\n",
    "    return seq_tensor.to(device), seq_lengths.to(device), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3c81e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义RNN分类器\n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_direction = 2 if bidirectional else 1\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size*self.n_direction, output_size)\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers*self.n_direction, batch_size, self.hidden_size)\n",
    "        return hidden.to(device)\n",
    "    \n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    "        gru_input = pack_padded_sequence(embedding, seq_lengths.cpu())\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        if self.n_direction == 2:\n",
    "            hidden_cat = torch.cat((hidden[-1], hidden[-2]), dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e18fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义训练函数\n",
    "def train():\n",
    "    total_loss = 0\n",
    "    for i, (phrase, sentiment) in enumerate(train_loader, 1):\n",
    "        inputs, seq_lengths, target = make_tensors(phrase, sentiment)\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch{epoch}', end='')\n",
    "            print(f'[{i * len(inputs)}/{len(train_set)}]', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a7c4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModel():\n",
    "    correct = 0\n",
    "    total = len(validation_set)\n",
    "    print(\"Evaluating trained model...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (phrase, sentiment) in enumerate(validation_loader, 1):\n",
    "            inputs, seq_lengths, target = make_tensor(phrase, sentiment)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            pred = output.max(dim=1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4dda5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取测试集\n",
    "def get_test_set():\n",
    "    test_set = pd.read_csv('./data/test.tsv', '\\t')\n",
    "    PhraseId = test_set['PhraseId']\n",
    "    Phrase = test_set['Phrase']\n",
    "    return PhraseId, Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ffcb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为测试集写的处理文本函数\n",
    "def make_tensor_test(phrase):\n",
    "    sequences_and_lengths = [phrase2list(phrase) for phrase in phrase]  # 名字字符串->字符数组->对应ASCII码\n",
    "    phrase_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])\n",
    "\n",
    "    # make tensor of name, batchSize x seqLen\n",
    "    seq_tensor = torch.zeros(len(phrase_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(phrase_sequences, seq_lengths)):  # 填充零\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)  # name_sequences不够最大长度的位置补零\n",
    "\n",
    "    # 排序 sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)  # perm_idx表示排完序元素原本的索引\n",
    "    seq_tensor = seq_tensor[perm_idx]  # 对补零后的name_sequences按照长度排序\n",
    "    # 因为这里将测试集的每个Batch的文本顺序打乱了，记录原本的顺序org_idx，以便将预测出的结果顺序还原\n",
    "    _, org_idx = perm_idx.sort(descending=False)\n",
    "    return create_tensor(seq_tensor), create_tensor(seq_lengths), org_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62f3e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    # 使用模型得到结果\n",
    "    PhraseId, Phrase = get_test_set()  # 获取测试集\n",
    "    sentiment_list = []  # 定义预测结果列表\n",
    "    batchNum = math.ceil(PhraseId.shape[0] / BATCH_SIZE)  # 获取总的Batch数\n",
    "    classifier = torch.load('./sentimentAnalyst.pkl')\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        classifier.to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(batchNum):\n",
    "            print(i)\n",
    "            if i == batchNum - 1:\n",
    "                phraseBatch = Phrase[BATCH_SIZE * i:]  # 处理最后不足BATCH_SIZE的情况\n",
    "            else:\n",
    "                phraseBatch = Phrase[BATCH_SIZE * i:BATCH_SIZE * (i + 1)]\n",
    "            inputs, seq_lengths, org_idx = make_tensor_test(phraseBatch)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            sentiment = output.max(dim=1, keepdim=True)[1]\n",
    "            sentiment = sentiment[org_idx].squeeze(1)\n",
    "            sentiment_list.append(sentiment.cpu().numpy().tolist())\n",
    "\n",
    "    sentiment_list = list(chain.from_iterable(sentiment_list))  # 将sentiment_list按行拼成一维列表\n",
    "    result = pd.DataFrame({'PhraseId': PhraseId, 'Sentiment': sentiment_list})\n",
    "    result.to_csv('./Submission_Validation.csv', index=False)  # 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8aa0ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "453adbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASS, N_LAYER).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "292a2c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n",
      "Epoch1[1280/124848]loss=0.010294249933212995\n",
      "Epoch1[2560/124848]loss=0.01009270278736949\n",
      "Epoch1[3840/124848]loss=0.010089941415935754\n",
      "Epoch1[5120/124848]loss=0.0100356305250898\n",
      "Epoch1[6400/124848]loss=0.010044892020523548\n",
      "Epoch1[7680/124848]loss=0.010016856264943879\n",
      "Epoch1[8960/124848]loss=0.009966118207999638\n",
      "Epoch1[10240/124848]loss=0.00995029219193384\n",
      "Epoch1[11520/124848]loss=0.009932839942889082\n",
      "Epoch1[12800/124848]loss=0.009914911380037665\n",
      "Epoch1[14080/124848]loss=0.009889365055344322\n",
      "Epoch1[15360/124848]loss=0.00985811953432858\n",
      "Epoch1[16640/124848]loss=0.00983552337409212\n",
      "Epoch1[17920/124848]loss=0.009811528419543589\n",
      "Epoch1[19200/124848]loss=0.009773181571314732\n",
      "Epoch1[20480/124848]loss=0.009746375743998215\n",
      "Epoch1[21760/124848]loss=0.009726351521470968\n",
      "Epoch1[23040/124848]loss=0.009683918890853722\n",
      "Epoch1[24320/124848]loss=0.00966422650963068\n",
      "Epoch1[25600/124848]loss=0.00964702595025301\n",
      "Epoch1[26880/124848]loss=0.009616424396101917\n",
      "Epoch1[28160/124848]loss=0.009615820849483663\n",
      "Epoch1[29440/124848]loss=0.009606573710461025\n",
      "Epoch1[30720/124848]loss=0.009595729853026569\n",
      "Epoch1[32000/124848]loss=0.009577586598694324\n",
      "Epoch1[33280/124848]loss=0.009556875119988735\n",
      "Epoch1[34560/124848]loss=0.00955102641687349\n",
      "Epoch1[35840/124848]loss=0.009538554726168514\n",
      "Epoch1[37120/124848]loss=0.009527454717919744\n",
      "Epoch1[38400/124848]loss=0.009518903174127142\n",
      "Epoch1[39680/124848]loss=0.009494026017285162\n",
      "Epoch1[40960/124848]loss=0.0094775878242217\n",
      "Epoch1[42240/124848]loss=0.00947100230859536\n",
      "Epoch1[43520/124848]loss=0.009458106769906247\n",
      "Epoch1[44800/124848]loss=0.009443508120519774\n",
      "Epoch1[46080/124848]loss=0.00942919799644086\n",
      "Epoch1[47360/124848]loss=0.009416288854806004\n",
      "Epoch1[48640/124848]loss=0.009404093563850772\n",
      "Epoch1[49920/124848]loss=0.009394605340770423\n",
      "Epoch1[51200/124848]loss=0.009382515654433519\n",
      "Epoch1[52480/124848]loss=0.009369567076380297\n",
      "Epoch1[53760/124848]loss=0.009354324994741805\n",
      "Epoch1[55040/124848]loss=0.009340814746872976\n",
      "Epoch1[56320/124848]loss=0.00933378412684595\n",
      "Epoch1[57600/124848]loss=0.009323404394090176\n",
      "Epoch1[58880/124848]loss=0.009306303154596168\n",
      "Epoch1[60160/124848]loss=0.00929899892195108\n",
      "Epoch1[61440/124848]loss=0.00928202007295719\n",
      "Epoch1[62720/124848]loss=0.009278027341720096\n",
      "Epoch1[64000/124848]loss=0.00926479980070144\n",
      "Epoch1[65280/124848]loss=0.009259068495685271\n",
      "Epoch1[66560/124848]loss=0.00924674701393367\n",
      "Epoch1[67840/124848]loss=0.009235188610992342\n",
      "Epoch1[69120/124848]loss=0.0092224604010375\n",
      "Epoch1[70400/124848]loss=0.009206261875277216\n",
      "Epoch1[71680/124848]loss=0.00919573296393667\n",
      "Epoch1[72960/124848]loss=0.009182306672364735\n",
      "Epoch1[74240/124848]loss=0.009174074209295214\n",
      "Epoch1[75520/124848]loss=0.009162295795977115\n",
      "Epoch1[76800/124848]loss=0.009153291179488102\n",
      "Epoch1[78080/124848]loss=0.009143469224638138\n",
      "Epoch1[79360/124848]loss=0.009131533212418998\n",
      "Epoch1[80640/124848]loss=0.009119215666774719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m acc_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, N_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     acc \u001b[38;5;241m=\u001b[39m evalModel()\n\u001b[0;32m      7\u001b[0m     acc_list\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (phrase, sentiment) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     inputs, seq_lengths, target \u001b[38;5;241m=\u001b[39m make_tensors(phrase, sentiment)\n\u001b[1;32m----> 6\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[32], line 22\u001b[0m, in \u001b[0;36mRNNClassifier.forward\u001b[1;34m(self, input, seq_lengths)\u001b[0m\n\u001b[0;32m     20\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     21\u001b[0m gru_input \u001b[38;5;241m=\u001b[39m pack_padded_sequence(embedding, seq_lengths\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m---> 22\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgru_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     24\u001b[0m     hidden_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:1001\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    998\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    999\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1001\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1003\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1004\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "acc_list = []\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train()\n",
    "    acc = evalModel()\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    # 保存最优时的模型 ################################\n",
    "    if acc >= max(acc_list):\n",
    "        torch.save(classifier, './results/sentimentAnalyst.pkl')\n",
    "        print('Save Model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0acad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()\n",
    "epoch = [epoch + 1 for epoch in range(len(acc_list))]\n",
    "plt.plot(epoch, acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()  # 在测试集上预测结果\n",
    "# Plot Accuracy\n",
    "epoch = [epoch + 1 for epoch in range(len(acc_list))]\n",
    "plt.plot(epoch, acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning2023",
   "language": "python",
   "name": "deeplearning2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
