{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d530b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentiment Analysis on Movie Reviews'''\n",
    "import math\n",
    "import torch\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "698c6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAData(Dataset):\n",
    "    def __init__(self, train):\n",
    "        # 构建数据样本\n",
    "        self.train = train\n",
    "        self.data = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "\n",
    "        if self.train:\n",
    "            # 随机选取80%作为训练集，不可按索引顺序取，数据会不全面\n",
    "            self.data = self.data.sample(frac=0.8, replace=False, random_state=1, axis=0)\n",
    "            # self.data = self.data[:int(self.data.shape[0] * 0.8)]\n",
    "            self.data = self.data.reset_index(drop=True)  # 重新生成索引\n",
    "            ### 正式训练要训练所有数据 ###\n",
    "            # self.data = self.data\n",
    "            self.len = self.data.shape[0]\n",
    "        else:\n",
    "            # 20%作为验证集\n",
    "            self.data = self.data.sample(frac=0.2, replace=False, random_state=1, axis=0)\n",
    "            # self.data = self.data[int(self.data.shape[0] * 0.8):]\n",
    "            self.data = self.data.reset_index(drop=True)  # 重新生成索引\n",
    "            self.len = self.data.shape[0]\n",
    "        self.x_data, self.y_data = self.data['Phrase'], self.data['Sentiment']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据数据索引获取样本\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据长度\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce9852cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集验证集数据对象\n",
    "train_set = SAData(train=True)\n",
    "validation_set = SAData(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa32c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "N_CHARS = 128  # ASCII码个数\n",
    "HIDDEN_SIZE = 128\n",
    "N_LAYER = 2\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 100\n",
    "USE_GPU = True\n",
    "N_CLASS = len(set(train_set.y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "365ec7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集验证集数据加载对象\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    # num_workers=2\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    dataset=validation_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # 测试集不打乱有利于观察结果\n",
    "    # num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c56da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def phrase2list(phrase):\n",
    "    arr = [ord(c) for c in phrase]  # ord() 返回对应的ASCII码\n",
    "    return arr, len(arr)\n",
    "\n",
    "\n",
    "def create_tensor(tensor):\n",
    "    if USE_GPU:\n",
    "        device = torch.device('cuda:0')\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def make_tensor(phrase, sentiment):\n",
    "    sequences_and_lengths = [phrase2list(phrase) for phrase in phrase]  # 名字字符串->字符数组->对应ASCII码\n",
    "    phrase_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths]).cpu()\n",
    "    sentiment = sentiment.long()\n",
    "\n",
    "    # make tensor of name, batchSize x seqLen\n",
    "    seq_tensor = torch.zeros(len(phrase_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(phrase_sequences, seq_lengths)):  # 填充零\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)  # name_sequences不够最大长度的位置补零\n",
    "\n",
    "    # 排序 sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)  # perm_idx表示排完序元素原本的索引\n",
    "    seq_tensor = seq_tensor[perm_idx]  # 对补零后的name_sequences按照长度排序\n",
    "    sentiment = sentiment[perm_idx]\n",
    "\n",
    "    return create_tensor(seq_tensor), create_tensor(seq_lengths), create_tensor(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3c81e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirection=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirection else 1\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirection)\n",
    "        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)\n",
    "        return create_tensor(hidden)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()  # 转置 B x S -> S x B\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        # 这里的pack，理解成压紧比较好。\n",
    "        # 将一个 填充过的变长序列 压紧。（填充时候，会有冗余，所以压紧一下）\n",
    "        gru_input = pack_padded_sequence(embedding, seq_lengths)  # pack them up\n",
    "\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        if self.n_directions == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "\n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e18fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    total_loss = 0\n",
    "    for i, (phrase, sentiment) in enumerate(train_loader, 1):\n",
    "        inputs, seq_lengths, target = make_tensor(phrase, sentiment)\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'[{time_since(start)}] Epoch {epoch}', end='')\n",
    "            print(f'[{i * len(inputs)}/{len(train_set)}]', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a7c4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModel():\n",
    "    correct = 0\n",
    "    total = len(validation_set)\n",
    "    print(\"Evaluating trained model...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (phrase, sentiment) in enumerate(validation_loader, 1):\n",
    "            inputs, seq_lengths, target = make_tensor(phrase, sentiment)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            pred = output.max(dim=1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4dda5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取测试集\n",
    "def get_test_set():\n",
    "    test_set = pd.read_csv('./data/test.tsv', '\\t')\n",
    "    PhraseId = test_set['PhraseId']\n",
    "    Phrase = test_set['Phrase']\n",
    "    return PhraseId, Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ffcb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为测试集写的处理文本函数\n",
    "def make_tensor_test(phrase):\n",
    "    sequences_and_lengths = [phrase2list(phrase) for phrase in phrase]  # 名字字符串->字符数组->对应ASCII码\n",
    "    phrase_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])\n",
    "\n",
    "    # make tensor of name, batchSize x seqLen\n",
    "    seq_tensor = torch.zeros(len(phrase_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(phrase_sequences, seq_lengths)):  # 填充零\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)  # name_sequences不够最大长度的位置补零\n",
    "\n",
    "    # 排序 sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)  # perm_idx表示排完序元素原本的索引\n",
    "    seq_tensor = seq_tensor[perm_idx]  # 对补零后的name_sequences按照长度排序\n",
    "    # 因为这里将测试集的每个Batch的文本顺序打乱了，记录原本的顺序org_idx，以便将预测出的结果顺序还原\n",
    "    _, org_idx = perm_idx.sort(descending=False)\n",
    "    return create_tensor(seq_tensor), create_tensor(seq_lengths), org_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62f3e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    # 使用模型得到结果\n",
    "    PhraseId, Phrase = get_test_set()  # 获取测试集\n",
    "    sentiment_list = []  # 定义预测结果列表\n",
    "    batchNum = math.ceil(PhraseId.shape[0] / BATCH_SIZE)  # 获取总的Batch数\n",
    "    classifier = torch.load('./sentimentAnalyst.pkl')\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        classifier.to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(batchNum):\n",
    "            print(i)\n",
    "            if i == batchNum - 1:\n",
    "                phraseBatch = Phrase[BATCH_SIZE * i:]  # 处理最后不足BATCH_SIZE的情况\n",
    "            else:\n",
    "                phraseBatch = Phrase[BATCH_SIZE * i:BATCH_SIZE * (i + 1)]\n",
    "            inputs, seq_lengths, org_idx = make_tensor_test(phraseBatch)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            sentiment = output.max(dim=1, keepdim=True)[1]\n",
    "            sentiment = sentiment[org_idx].squeeze(1)\n",
    "            sentiment_list.append(sentiment.cpu().numpy().tolist())\n",
    "\n",
    "    sentiment_list = list(chain.from_iterable(sentiment_list))  # 将sentiment_list按行拼成一维列表\n",
    "    result = pd.DataFrame({'PhraseId': PhraseId, 'Sentiment': sentiment_list})\n",
    "    result.to_csv('./Submission_Validation.csv', index=False)  # 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "453adbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASS, N_LAYER)\n",
    "if USE_GPU:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    classifier.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "292a2c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m acc_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, N_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     acc \u001b[38;5;241m=\u001b[39m evalModel()\n\u001b[0;32m      7\u001b[0m     acc_list\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (phrase, sentiment) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      4\u001b[0m     inputs, seq_lengths, target \u001b[38;5;241m=\u001b[39m make_tensor(phrase, sentiment)\n\u001b[1;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[35], line 25\u001b[0m, in \u001b[0;36mRNNClassifier.forward\u001b[1;34m(self, input, seq_lengths)\u001b[0m\n\u001b[0;32m     21\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 这里的pack，理解成压紧比较好。\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 将一个 填充过的变长序列 压紧。（填充时候，会有冗余，所以压紧一下）\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m gru_input \u001b[38;5;241m=\u001b[39m \u001b[43mpack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lengths\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pack them up\u001b[39;00m\n\u001b[0;32m     27\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(gru_input, hidden)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_directions \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\utils\\rnn.py:263\u001b[0m, in \u001b[0;36mpack_padded_sequence\u001b[1;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[0;32m    259\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mindex_select(batch_dim, sorted_indices)\n\u001b[0;32m    262\u001b[0m data, batch_sizes \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m--> 263\u001b[0m     \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "acc_list = []\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    trainModel()\n",
    "    acc = evalModel()\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    # 保存最优时的模型 ################################\n",
    "    if acc >= max(acc_list):\n",
    "        torch.save(classifier, './results/sentimentAnalyst.pkl')\n",
    "        print('Save Model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()  # 在测试集上预测结果\n",
    "# Plot Accuracy\n",
    "epoch = [epoch + 1 for epoch in range(len(acc_list))]\n",
    "plt.plot(epoch, acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning2023",
   "language": "python",
   "name": "deeplearning2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
