# 模型训练与结果分析

## 房价预测

### 线性预测模型

```Python
loss = nn.MSELoss()
in_features = train_features.shape[1]  

def get_net():
    net = nn.Sequential(nn.Linear(in_features, 1))
    # 模型参数初始化
    for param in net.parameters():
        nn.init.normal_(param, mean=0, std=0.01)
    return net
```

![image-20230613205432973](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205432973.png)

![image-20230613205446730](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205446730.png)

![image-20230613205527775](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205527775.png)

### 普通MLP预测模型

```Python
# 设置超参数
input_dim = train_features.shape[1]
output_dim = 1
hidden_dim = 512
lr = 0.001
num_epochs = 500

# 初始化模型、损失函数以及优化器
model = Net(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# 训练MLP模型
for epoch in range(num_epochs):
    # 前向传播
    outputs = model(train_features)
    # 计算损失
    loss = criterion(outputs, train_labels)
    # 反向传播及优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    train_losses.append(loss.item())

    # 每10轮输出一次损失
    if epoch % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

```

![image-20230613205621854](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205621854.png)

![image-20230613205627123](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205627123.png)

### 带Dropout的MLP

```Python
# 设置超参数
input_dim = train_features.shape[1]
output_dim = 1
hidden_dim = 256
lr = 0.001
num_epochs = 1000

# 初始化模型、损失函数以及优化器
model = Net(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# 记录训练过程的指标
train_losses = []
# 训练MLP模型
for epoch in range(num_epochs):
    # 前向传播
    outputs = model(train_features)
    # 计算损失
    loss = criterion(outputs, train_labels)
    # 反向传播及优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    train_losses.append(loss.item())

    # 每10轮输出一次损失
    if epoch % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))
```

![image-20230613205705258](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205705258.png)

### LSTM模型

```Python
# 设置超参数
input_dim = train_features.shape[1]
output_dim = 1
hidden_dim = 512
num_layers = 2
lr = 0.001
num_epochs = 500

# 初始化模型、损失函数以及优化器
model = Net(input_dim, hidden_dim, num_layers, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=lr)
```

![image-20230613205742397](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205742397.png)

### GRU模型

```Python
# 定义GRU模型
class Net(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, batch_first=True):
        super(Net, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=batch_first)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # 初始化隐层
        h0 = torch.randn(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()
        # 将隐层传入GRU模型
        out, _ = self.gru(x, h0.detach())
        # 将输出特征传入全连接层
        out = self.fc(out[:, -1, :])
        return out

# 设置超参数
input_dim = train_features.shape[2]
output_dim = 1
hidden_dim = 128
lr = 0.001
num_epochs = 500

# 初始化模型、损失函数以及优化器
model = Net(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr)
```

![image-20230613205818048](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205818048.png)

![image-20230613205840401](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205840401.png)

![image-20230613205850952](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205850952.png)

## 影视评论情感分类训练

```Python
#定义训练函数
def train():
    total_loss = 0
    for i, (phrase, sentiment) in enumerate(train_loader, 1):
        inputs, seq_lengths, target = make_tensors(phrase, sentiment)
        output = classifier(inputs, seq_lengths)
        loss = criterion(output, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        if i % 10 == 0:
            print(f'Epoch{epoch}', end='')
            print(f'[{i * len(inputs)}/{len(train_set)}]', end='')
            print(f'loss={total_loss / (i * len(inputs))}')
    return total_loss
```

![image-20230613205934848](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205934848.png)

![image-20230613205943945](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613205943945.png)

![image-20230613210018219](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210018219.png)

## 泰坦尼克号存活率预测问题

```Python
epochs=10
loss_fn = nn.BCELoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.01)
for epoch in range(epochs + 1):
    for batch_idx, samples in enumerate(train_dataset):
        x_train, y_train = samples
        optimizer.zero_grad()
        prediction = model(x_train)
        cost = loss_fn(prediction, y_train)
        cost.backward()
        optimizer.step()
        
        if batch_idx%250 == 0:
            print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(
                epoch, epochs, batch_idx+1, len(train_dataset),
                cost.item()
                ))
    validation_data_eval = []
    for batch_idx, samples in enumerate(val_dataset):
        x_train, y_train = samples
        prediction = model(x_train)
        cost = loss_fn(prediction, y_train)
        validation_data_eval.append(cost.item())
    print("validation cost : ", np.mean(validation_data_eval))

```

```Python
input_dim = 1730
output_dim = 2
learning_rate = 1
model = LinearRegression(input_dim,output_dim)
error = nn.CrossEntropyLoss()  #交叉熵损失
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.5)

for iteration in range(iteration_number):
    batch_loss = 0
    batch_accur = 0
    temp = 0

    for (x, y) in generate_batches(X_train, y_train, batch_size):
        inputs = Variable(torch.from_numpy(x)).float()
        labels = Variable(torch.from_numpy(y))
            
        optimizer.zero_grad() 

        results = model(inputs)
        
        loss = error(results, labels)

        batch_loss += loss.data
        
        loss.backward()
        
        optimizer.step()

        with torch.no_grad():
            _, pred = torch.max(results, 1)
            batch_accur += torch.sum(pred == labels)
            temp += len(pred)
    
    loss_list.append(batch_loss/batch_no)
    acc_list.append(batch_accur/temp)
    
    if(iteration % 50 == 0):
        print('epoch {}: loss {}, accuracy {}'.format(iteration, batch_loss/batch_no, batch_accur/temp))
```

![image-20230613210135965](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210135965.png)

![image-20230613210147721](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210147721.png)

![image-20230613210153912](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210153912.png)

![image-20230613210229981](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210229981.png)

![image-20230613210246263](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210246263.png)

## 股票问题

```Python
# 指定保存日志的路径和名称
writer = SummaryWriter(log_dir='./logs')
for i in range(epochs):
    total_loss = 0
    for idx, (data, label) in enumerate(train_loader):
        if useGPU:
            data1 = data.squeeze(1).cuda()
            pred = model(Variable(data1).cuda())
            # print(pred.shape)
            pred = pred[1,:,:]
            label = label.unsqueeze(1).cuda()
            # print(label.shape)
        else:
            data1 = data.squeeze(1)
            pred = model(Variable(data1))
            pred = pred[1, :, :]
            label = label.unsqueeze(1)
        loss = criterion(pred, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
    mean_loss = total_loss / len(train_loader.dataset)
    writer.add_scalar('Train/Loss', mean_loss, i)
    print(total_loss)
    if i % 10 == 0:
        # torch.save(model, args.save_file)
        torch.save({'state_dict': model.state_dict()}, './weights/stock.pkl')
        print('第%d epoch，保存模型' % i)
writer.close()
# torch.save(model, args.save_file)
torch.save({'state_dict': model.state_dict()}, './weights/stock.pkl')
```

![image-20230613210327530](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210327530.png)

![image-20230613210315370](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210315370.png)

## BERT+Transformer+BiLSTM

```Python
## 设置预训练超参数
batch_size = 4
device = 'cuda' if torch.cuda.is_available() else 'cpu'
epochs = 10   # 训练轮次
learning_rate = 5e-6   #学习率设置的比较低


for epoch in range(1,epochs+1):
    losses = 0  #损失
    accuracy = 0  # 准确率
    BERT.train()   #训练
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    train_bar = tqdm(train_dataloader, ncols=100)
    for input_ids,token_type_ids,attention_mask,label_id in train_bar:
        #梯度清零
        BERT.zero_grad()
        train_bar.set_description('Epoch %i train' % epoch)
        
        #传入数据 调用 model.forward()
        output = BERT(
            input_ids=input_ids.to(device),
            attention_mask=attention_mask.to(device),
            token_type_ids=token_type_ids.to(device))
        
        #计算loss
        loss= criterion(output,label_id.to(device))
        losses += loss.item()
        
        pred_labels = torch.argmax(output,dim=1)  #预测的label
        acc = torch.sum(pred_labels == label_id.to(device)).item() / len(pred_labels) #acc
        accuracy += acc
        
        loss.backward()
        optimizer.step()
        train_bar.set_postfix(loss = loss.item(),acc=acc)
    average_loss = losses / len(train_dataloader)
    average_acc = accuracy / len(train_dataloader)
    
    print('\tTrain ACC:', average_acc, '\tLoss:', average_loss)
    
    # 保存训练集的loss和accuracy供后续可视化
    train_losses.append(average_loss)
    train_accs.append(average_acc)
    
    # 验证
    model.eval()
    losses = 0  # 损失
    pred_labels = []
    true_labels = []
    valid_bar = tqdm(valid_dataloader, ncols=100)
    for input_ids, token_type_ids, attention_mask, label_id in valid_bar:
        valid_bar.set_description('Epoch %i valid' % epoch)

        output = model(
            input_ids=input_ids.to(device),
            attention_mask=attention_mask.to(device),
            token_type_ids=token_type_ids.to(device),
        )

        loss = criterion(output, label_id.to(device))
        losses += loss.item()

        pred_label = torch.argmax(output, dim=1)  # 预测出的label
        acc = torch.sum(pred_label == label_id.to(device)).item() / len(pred_label)  # acc
        valid_bar.set_postfix(loss=loss.item(), acc=acc)

        pred_labels.extend(pred_label.cpu().numpy().tolist())
        true_labels.extend(label_id.numpy().tolist())

    average_loss = losses / len(valid_dataloader)
    print('\tLoss:', average_loss)
    # 保存验证集的loss供后续可视化
    valid_losses.append(average_loss)
    
    #分类报告
    report = metrics.classification_report(true_labels, pred_labels, labels=valid_dataset.labels_id,
                                               target_names=valid_dataset.labels)
    print('* Classification Report:')
    print(report)
    
    # f1 用来判断最优模型
    f1 = metrics.f1_score(true_labels, pred_labels, labels=valid_dataset.labels_id, average='micro')
    
    if not os.path.exists('models'):
        os.makedirs('models')
    
    #判断并保存验证集上表现最好的模型
    if f1 > best_f1:
        best_f1 = f1
        print("找到了更好的模型")
        torch.save(BERT.state_dict(),'models/best_model.pkl')
```

![image-20230613210445480](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613210445480.png)

![image-20230613213111356](https://happygoing.oss-cn-beijing.aliyuncs.com/image-20230613213111356.png)